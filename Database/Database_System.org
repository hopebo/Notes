* 1 Database System
Organized collection of inter-related data that models some aspect of the real-world.
** 1.1 Data models
Most DBMS:
- Relational

NoSQL:
- Key / Value
- Graph
- Document
- Column-family

Machine Learning:
- Array / Matrix

Others:
- Hierarchical
- Network
** 1.2 Relational Model
A *relation* is a unordered set that contain the relationship of attributes that represent entities.

A *tuple* is a set of attribute values (also known as its domain) in the relation.
- The special value NULL is a member of each domain.

A relation's *primary key* uniquely identifies a single tuple.

A *foreign key* specifies that an attribute from one relation has to map to a tuple in another relation.
* 2 Advanced SQL
** 2.1 SQL(sequel)
SQL is a collection of DML(Data Manipulation Language), DDL(Data Definition Language), DCL(Data Control Language).

- DML

Commands like insert update delete select, the things that actually manipulate the data.

- DDL

Create tables, define schemas.

- DCL

Security authorization.

SQL is based on bags (duplicates) not sets (no duplicates).
** 2.2 Aggregates
Functions that return a single value from a bag(unordered, duplicates allowed) of tuples: Max, Min, Avg, Count.

Aggregate functions can only be used in the *SELECT* ouput list.

COUNT, SUM, AVG support *DISTINCT*.

Output of other columns outside of an aggregate is undefined. For example:
#+begin_src sql
SELECT AVG(s.gpa), e.cid
  FROM enrolled AS e, student AS s
 WHERE e.sid = s.sid
#+end_src
The above =e.cid= is undefined.
** 2.3 GROUP BY
Project tuples into subsets and calculate aggregates against each subset.

*Non-aggregated* values in SELECT output clause *must* appear in GROUP BY clause.
** 2.4 HAVING
Filters results based on aggregation computation. Like a WHERE clause for a GROUP BY.
** 2.5 String Operations
|         | String Case | String Quotes |
|---------+-------------+---------------|
| Postgre | Sensitive   | Single Only   |
| MySQL   | Insensitive | Single/Double |
*** 2.5.1 LIKE
LIKE is used for string matching. String-matching operators:

- '%' Matches any substring (including empty strings).
- '-' Match any one character.
** 2.6 DATE/TIME Operations
Operations to manipulate and modify DATE/TIME attributes.

Can be used in either output and predicates.
** 2.7 Output Redirection
Store query results in another table:
- Table must not already be defined.
- Table will have the same # of columns with the same types as the input.

Insert tuples from query into another table:
- Inner SELECT must generate the same columns as the target table.
- DBMSs have different options/syntax on what to do with duplicates.
** 2.7 Output Control
*** 2.7.1 ORDER BY <column*> [ASC|DESC]
Order the output tuples by the values in one or more of their columns.
*** 2.7.2 Limit (Unsorted)
Limit the # of tuples returned in output.

Can set an offset to return a "range".
** 2.8 Nested Queries
Queries containing other queries. They are often difficult to optimize.

Inner queries can appear (almost) anywhere in query.

In this case:
#+begin_src sql
SELECT name FORM student
 WHERE sid IN (
   SELECT sid FROM enrolled
    WHERE cid = '15-445'
 )
#+end_src
For every single tuple in the outer query we execute the inner query over and over again.
** 2.9 Window Functions
Performs a calculation across a set of tuples that related to a single row. Like an aggregation but tuples are not grouped into a single output tuples.

#+begin_src sql
SELECT .. FUNC-NAME(...) OVER(...)
  FROM tableName
#+end_src

It's like combining the aggregation and the group by but in a single clause so the function is like the aggregation function and the over is like the group by.
- Aggregation functions
- Special window functions:
  - ROW_NUMBER() # of the current row.
  - RANK() Order position of the current row.

The *OVER* keyword specifies how to group together tuples when computing the window function.

Use *PARTITION BY* to specify group.
** 2.10 Common Table Expressions(CTE)
Provides a way to write auxiliaxy statements for use in a larger query.

- Think of it like a temp table just for one query.

Alternative to nested queries and views.

#+begin_src sql
WITH cteName (col1, col2) AS (
  SELECT 1, 2
)
SELECT col1 + col2 FROM cteName
#+end_src
* 3 Database Storage Part I
Problem: How the DBMS represents the database in files on disk.

*Query Planning* -> *Operator Execution* -> *Access Methods* -> *Buffer Pool Manager* -> *Disk Manager*

** 3.1 Access Times
| Device          | Time(ns)      |
|-----------------+---------------|
| L1 Cache Ref    | 0.5           |
| L2 Cache Ref    | 7             |
| DRAM            | 100           |
| SSD             | 150,000       |
| HDD             | 10,000,000    |
| Network Storage | ~30,000,000   |
| Tape Archives   | 1,000,000,000 |
** 3.2 Why not use the OS (mmp)?
DBMS (almost) always wants to control things itself and can do a better job at it.
- Flushing dirty pages to disk in the correct order.
- Specialized prefetching.
- Buffer replacement policy.
- Thread / process scheduling.
** 3.3 Database Pages
A page is a fixed-size block of data.
- It can contain tuples, meta-data, indexes, log records...
- Most systems do not mix page types.
- Some systems require a page to be self-contained.

Each page is given a unique identifier.
- The DBMS uses an indirection layer to map page ids to physical locations.

There are three different notions of "pages" in a DBMS:
- Hardware Page (usually 4KB)
- OS Page (usually 4KB)
- Database Page (1-16KB)

By hardware page, we mean at what level the device can guarantee a "failsafe write".

| DBMS                   | Database Page(KB) |
|------------------------+-------------------|
| SQLite                 |                 1 |
| DB2, ORACLE            |                 4 |
| SQL Server, PostgreSQL |                 8 |
| MySQL                  |                16 |
** 3.4 File Storage and Storage Manager
The DBMS stores a database as one or more files on disk.

The storage manager is responsible for maintaining a database's files. It organizes the files as a collection of pages.
- Tracks data read/written to pages.
- Tracks the available space.
** 3.5 Page File Architecture
*** 3.5.1 Heap File
A heap file is an unordered collection of pages where tuples that are stored in random order.
- Get / Delete page.
- Must also support iterating over all pages.

Need meta-data to keep track of what pages exist and which ones have free space.

Two ways to represent a heap file:
- Linked List
- Page Directory

**** 3.5.1.1 Heap File: Linked List
Maintain a header page at the beginning of the file that stores two pointers:
- HEAD of the free page list.
- HEAD of the data page list.

Each page keeps track of the number of free slots in itself.
**** 3.5.1.2 Heap File: Page Directory
The DBMS maintains special pages that tracks the location of data pages in the database files.

The directory also records the number of free slots per page.

The DBMS has to make sure the directory pages are in sync with the data pages.
*** 3.5.2 Page Layout
**** 3.5.2.1 Page Header
Every page contains a header of meta-data about the page's contents.
- Page Size
- Checksum
- DBMS Version
- Transaction Visibility
- Compression Information
**** 3.5.2.2 Page Layout
***** 3.5.2.2.1 Tuple-oriented
The most common layout scheme is called slotted pages. The slot array maps "slots" to the tuples' starting position offsets.

The header keeps track of:
- The # of used slots
- The offset of the starting location of the last slot used.
***** 3.5.2.2.2 Log-structured
Instead of storing tuples in pages, the DBMS only stores log records.

The system appends log records to the file of how the database was modified:
- Inserts store the entire tuple.
- Deletes mark the tuple as deleted.
- Updates contain the delta of just the attributes that were modified.

To read a record, the DBMS scans the log backwards and "recreates" the tuple to find what it needs.

Build indexes to allow it to jump to locations in the log.

Periodically compacting the logs coalesces larger log files into smaller files by removing unnecessary records.

*** 3.5.3 Tuple Layout
A tuple is essentially a sequence of bytes. It's the job of DBMS to interpret those bytes into attribute type and values.

Each tuple is prefixed with a header that contains meta-data about it.
- Visibility info (concurrency control)
- Bit Map for NULL values.

Attributes are typically stored in the order that you specify them when you create the table.

Can physically denormalize (e.g., "pre-join") related tuples and store them together in the same page.
- Potentially reduces the amount of I/O for common workload patterns.
- Can make updates more expensive.

**** 3.5.3.1 Record IDs
The DBMS needs a way to keep track of individual tuples.

Each tuple is assigned a unique record identifier.
- Most common: page_id + offset/slot.
- Can also contain file location info.
* 4 Database Storage Part II
The DBMS assumes that the primary storage location of the database is on non-volatile disk.

The DBMS's components manage the movement of data between non-volatile and volatile storage.
** 4.1 Data Representation
A tuple is essentially a sequence of bytes. It's the job of the DBMS to interpret those bytes into attribute types and values.

The DBMS's catalogs contain the schema information about tables that the system uses to figure out the tuple's layout.
*** 4.1.1 Data Types
INTEGER/BIGINT/SMALLINT/TINYINT
- C/C++ Representation

FLOAT/REAL vs. NUMERIC/DECIMAL
- IEEE-754 Standard / Fixed-point Decimals

VARCHAR/VARBINARY/TEXT/BLOB
- Header with length, followed by data bytes.

TIME/DATE/TIMESTAMP
- 32/64-bit integer of (micro) seconds since Unix epoch.

**** 4.1.1.1 Variable Precision Numbers
Inexact, variable-precision numeric type that uses the "native" C/C++ types. Store directly as specified by IEEE-754.

Typically faster than arbitary precision numbers.
**** 4.1.1.2 Fixed Precision Numbers
Numeric data types with arbitary precision and scale. Used when round errors are unacceptable.
- Example: NUMERIC, DECIMAL.

Typically stored in a exact, variable-length binary representation with additional meta-data.
- Like a VARCHAR but not stored as a string.
**** 4.1.1.3 Large Values
Most DBMSs don't allow a tuple to exceed the size of a single page.

To store values that are larger than a page, the DBMS uses separate overflow storage pages.
**** 4.1.1.4 External Value Storage
Some systems allow you to store a really large value in an external file. Treated as a BLOB type.

The DBMS cannot manipulate the contents of an external file.
- No durability protections.
- No transaction protections.

** 4.2 System Catalogs
A DBMS stores meta-data about databases in its internal catalogs.
- Table, columns, indexes, views
- Users, permissions
- Internal statistics

Almost every DBMS stores their databases' catalog in itself.
- Wrap object abstraction around tuples.
- Specialized code for "bootstrapping" catalog tables.

You can query the DBMS's internal INFORMATION_SCHEMA catalog to get info about the database.
- ANSI standard set of read-only views that provide info about all of the tables, views, columns, and procedures in a database.
** 4.3 Storage Models
*** 4.3.1 Workloads
On-line Transaction Processing:
- Simple queries that read/update a small amount of data that is related to a single entity in the database.

On-line Analytical Processing:
- Complex queries that read large portions of the database spanning multiple entities.

The DBMS can store tuples in different ways that are better for either OLTP or OLAP workloads.
*** 4.3.2 N-ary Storage Model (NSM)
Also known as row storage. The DBMS stores all attributes for a single tuple contiguously in a page.

Ideal for OLTP workloads where queries tend to operate only on an individual entity and insert-heavy workloads.

Advantages:
- Fast inserts, updates, and deletes.
- Good for queries that need the entire tuple.

Disadvantages:
- Not good for scanning large portions of the table and/or a subset of the attributes.
*** 4.3.3 Decomposition Storage Model (DSM)
Also known as column store. The DBMS stores the values of a single attribute for all tuples contiguously in a page.

Ideal for OLAP workloads where read-only queries perform large scans over a subset of the table's attributes.

Tuple Identifications:
- Fixed-length Offsets. Each value is the same length for an attribute.
- Embedded Tuple Ids. Each value is stored with its tuple id in a column.

Advantages:
- Reduces the amount wasted I/O because the DBMS only reads the data it needs.
- Better query processing and data compression.

Disadvantages:
- Slow for point queries, inserts, updates, and deletes because of tuple splitting/stitching.
* 5 Buffer Pools
Problem: How the DBMS manages its memory and move data back-and-forth from disk.

Spatial Control:
- Where to write pages on disk.
- The goal is to keep pages that are used together often as physically close together as possible on disk.

Temporal Control:
- When to read pages into memory, and when to write them to disk.
- The goal is minimize the number of stalls from having to read data from disk.
** 5.1 Buffer Pool Manager
*** 5.1.1 Buffer Pool Organization
Memory region organized as an array of fixed-size pages. An array entry is called a frame.

When the DBMS requests a page, an exact copy is placed into one of these frames.
*** 5.1.2 Buffer Pool Meta-data
The page table keeps track of pages that are currently in the memory.

Also maintains additional meta-data per page:
- Dirty Flag
- Pin/Reference Counter
*** 5.1.3 Locks VS. Latches
Locks:
- Protects the database's logical contents from other transactions.
- Held for transaction duration.
- Need to be able to rollback changes.

Latches (Mutex):
- Protects the critical sections of the DBMS's internal data structure from other threads.
- Held for operation duration.
- Do not need to be able to rollback changes.
*** 5.1.4 Page Table VS. Page Directory
The page directory is the mapping from page ids to page locations in the database files.
- All changes must be recorded on disk to allow the DBMS to find on restart.

The page table is the mapping from page ids to a copy of the page in buffer pool frames.
- This is an in-memory data structure that does not need to be stored on disk.
*** 5.1.5 Multiple Buffer Pools
The DBMS does not always have a single buffer pool for the entire system.
- Multiple buffer pool instances
- Per-database buffer pool
- Per-page type buffer pool

Helps reduce latch contention and improve locality.
*** 5.1.6 Pre-fetching and Scan Sharing
**** 5.1.6.1 Pre-fetching
The DBMS can also prefetch pages based on a query plan.
- Sequential Scans
- Index Scans
**** 5.1.6.2 Scan Sharing
Queries are able to reuse data retrieved from storage or operator computations.
- This is different from result caching.

Allow multiple queries to attach to a single cursor that scans a table.
- Queries do not have to be exactly the same.
- Can also share intermediate results.

If a query starts a scan and if there one already doing this, then the DBMS will attach to the second query's cursor.
- The DBMS keeps track of where the second query joined with the first so that it can finish the scan when it reaches the end of the data structure.

*** 5.1.7 OS Page Cache
Most disk operations go through the OS API. Unless you tell it not to, the OS maintains its own filesystem cache.

Most DBMSs use direct I/O (O_DIRECT) to bypass the OS's cache.
- Redundant copies of pages.
- Different eviction policies.
** 5.2 Buffer Replacement Policies
When the DBMS needs to free up a frame to make room for a new page, it must decide which page to evict from the buffer pool.

Goals:
- Correctness
- Accuracy
- Speed
- Meta-data overhead
*** 5.2.1 Least Recently Used
Maintain a timestamp of when each page was last accessed. When the DBMS needs to evict a page, select the one with oldest timestamp.
- Keep the pages in sorted order to reduce the search time on eviction.
*** 5.2.2 Clock
Approximation of LRU without needing a separate timestamp per page.
- Each page has a reference bit.
- When a page is accessed, set to 1.

Organize the pages in a circular buffer with a "clock hand":
- Upon sweeping, check if a page's bit is set to 1.
- If yes, set to zero. If no, then evict.

Problems:

LRU and Clock replacement policies are susceptible to sequential flooding.
- A query performs a sequential scan that reads every page.
- This pollutes the buffer pool with pages that are read once and then never again.

The most recently used page is actually the most unneeded page.
*** 5.2.3 Better Policies: LRU-K
Take into account history of the last K references as timestamps and compute the interval between subsequent accesses.

The DBMS then uses this history to estimate the next time that page is going to be accessed.
*** 5.2.4 Better Policies: Localization
The DBMS chooses which pages to evict on a per txn/query basis. This minimizes the pollution of the buffer pool from each query.
- Keep track of the pages that a query has accessed.
*** 5.2.5 Better Policies: Priority Hints
The DBMS knows what the context of each page during query execution.

It can provide hints to the buffer pool on whether a page is important or not.
** 5.3 Dirty Pages
FAST: If a page in the buffer pool is not dirty, then the DBMS can simply "drop" it.

SLOW: If a page is dirty, then the DBMS must write back to disk to ensure that its changes are persisted.

Trade-off between fast evictions versus dirty writing pages that will not be read again in the future.
*** 5.3.1 Background Writing
The DBMS can periodically walk through the page table and write dirty pages to disk.

When a dirty page is safely written, the DBMS can either evict the page or just unset the dirty flag.

Need to be careful that we don't write dirty pages before their log records have been written.
** 5.4 Allocation Policies
Global Policies:
- Make decisions for all active txns.

Local Policies:
- Allocate frames to a specific txn without considering the behavior of concurrent txns.
- Still need to support sharing pages.
** 5.5 Other Memory Pools
The DBMS needs memory for things other than just tuples and indexes.

These other memory pools may not always backed by disk. Depends on implementation.
- Sorting + Join Buffers
- Query Caches
- Maintenance Buffers
- Log Buffers
- Dictionary Caches
* 6 Hash Tables
Design Decisions:

Data Organization
- How we layout data structure in memory/pages and what information to store to support efficient access.

Concurrency
- How to enable multiple threads to access the data structure at the same time without causing problems.

A hash table implements an associative array abstract data type that maps keys to values.

It use a hash function to compute an offset into the array, from which the desired value can be found.
** 6.1 Hash Table
Design Decision #1: Hash Function
- How to map a large key space into a smaller domain.
- Trade-off between being fast vs. collision rate.

Design Decision #2: Hashing Scheme
- How to handle key collision after hashing.
- Trade-off between allocating a large hash table vs. additional instructions to find/insert keys.
** 6.2 Hash Functions
MurmurHash(2008)
- Designed to a fast, general purpose hash function.

Google CityHash(2011)
- Based on ideas from MurmurHash2
- Designed to be faster for short keys (<64 bytes).

Google FarmHash(2014)
- Newer version of CityHash with better collision rates.

CLHash(2016)
- Fast hashing function based on carry-less multiplication.
** 6.3 Hash Schemes
*** 6.3.1 Static Hashing Schemes
**** 6.3.1.1 Linear Probe Hasing
Single gaint table of slots.

Resolve collisions by linearly searching for the next free slot in the table.
- To determine whether an element is present, hash to a location in the index and scan for it.
- Have to store the key in the index to know when to stop scanning.
- Insertions and deletions are generalizations of lookups.
***** 6.3.1.1.1 Non-unique Keys
Choice #1: Separate Linked List
- Store values in separate storage area for each key.

Choice #2: Redundant Keys
- Store duplicate keys entries together in the hash table.
***** 6.3.1.1.2 Observation
To reduce the # of wasteful comparisons, it is important to avoid collisions of hashed keys.

This requires a hash table with ~2x the number of slots as the number of elements.
**** 6.3.1.2 Robin Hood Hashing
Variant of linear hashing that steals slots from "rich" keys and give them to "poor" keys.
- Each key tracks the number of positions they are from where its optimal position in the table.
- On insert, a key take the slot of another key if the first key is farther away from its optimal position than the second key.
**** 6.3.1.3 Cuckoo Hashing
Use multiple hash tables with different hash functions.
- On insert, check every table and pick anyone that has a free slot.
- If no table has a free slot, evict the element from one of them and then re-hash it find a new location.

Look-ups and deletions are always O(1) because only one location per hash table is checked.

Make sure that we don't get stuck in an infinite loop when moving keys.

If we find a cycle, then we can rebuild the entire hash tables with new hash functions.
- With two hash functions, we (probably) won't need to rebuild the table until it is at about 50% full.
- With three hash functions, we (probably) won't need to rebuild the table until it is at about 90% full.
*** 6.3.2 Daynamic Hashing Schemes
The previous hash tables require knowing the number of elements you want to store ahead of time.
- Otherwise you have rebuild the entire table if you need to grow/shrink.

Dynamic hash tables are able to grow/shrink on demand.
**** 6.3.2.1 Chained Hashing
Maintain a linked list of buckets for each slot in the hash table.

Resolve collisions by placing all elements with the same hash key into the same bucket.
- To determine whether an element is present, hash to its bucket and scan for it.
- Insertions and deletions are generalizations of lookups.

The hash table can grow infinitely because you just keep adding new buckets to the linked list.

You only need to take a latch on the bucket to store a new entry or extend the linked list.
**** 6.3.2.2 Extendible Hashing
Chained-hashing approach where we split buckets instead of letting the linked list grow forever.

This requires reshuffling entries on split, but the change is localized.
**** 6.3.2.3 Linear Hashing
Maintain a pointer that tracks the next bucket to split.

When any bucket overflow, split the bucket at the pointer location.

Overflow criterion is left up to the implementation.
- Space Utilization
- Average Length of Overflow Chains

Splitting buckets based on the split pointer will eventually get to all overflowed buckets.
- When the pointer reaches the last slot, delete the first hash function and move back to beginning.

The pointer can also move backwards when buckets are empty.
** 6.4 Conclusion
Fast data structures that support O(1) look-ups that are used all throughout the DBMS internals.
- Trade-off between speed and flexibility.

Hash tables are usually not what you want to for a table index.
* 7 Tree Indexes Part I
** 7.1 Table Indexes
A table index is a replica of a subset of a table's columns that are organized and/or sorted for efficient access using a subset of those columns.

The DBMS ensures that the contents of the table and the index are logically in sync.

It's the DBMS's job to figure out the best index(es) to use to execute each query.

There is a trade-off on the number of indexes to create per database.
- Storage Overhead
- Maintenance Overhead
** 7.2 B+ Tree
A B+ Tree is a self-balancing tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions in O(log n).
- Generalization of a binary search tree in that a node can have more than two children.
- Optimized for systems that read and write large blocks of data.
*** 7.2.1 Properties
A B+tree is an M-way(M is maximum number of keys you can have in a single node)search tree with the following properties:

- It is perfectly balanced (i.e., every leaf node is at the same depth).
- Every inner node other than the root, is at least half-full M/2-1 <= #keys <= M-1
- Every inner node with k keys has k+1 non-null children.
*** 7.2.2 B+ Tree Nodes
Every node in the B+tree contains an array of key/value pairs.
- The keys will always be the column or columns that you built your index on.
- The values will differ based on whether the node is classified as inner nodes(pointer to another inner node or leaf node) or leaf nodes(record id or the tuple of contents).

The arrays are always kept in sorted order. When you load on a node, you do a binary search.
**** 7.2.2.1 Leaf Node Values
Approach #1: Record Ids
- A pointer to the location of the tuple that the index entry corresponds to.

Approach #2: Tuple Data
- The actual contents of the tuple is stored in the leaf node.
- Secondary indexes have to store the record id as their values.
- Typically it's used for primary key.

The size of B+tree node is usally the size of page. The sibling pointers are not pointing to memories, instead page ids. Because database need to go to the page table to fetch the page.
**** 7.2.2.2 Type
Clustered B+ tree
- The sorted tuples are stored sequentially in the same page.

Unclustered B+ tree
- The tuples in leaf nodes are distributed in different pages.
**** 7.2.2.3 B Tree VS. B+ Tree
The original B Tree from 1972 stored keys + values in all nodes in the tree.
- More space efficient since each key only appears once in the tree.

A B+ Tree only stores values in leaf nodes. Inner nodes only guide the search process.
** 7.3 Operations
*** 7.3.1 Insert
Find correct leaf L.

Put data entry into L in sorted order.
- If L has enough space, done!
- Else, must split L into L and a new node L2
  - Redistribute entries evenly, copy up middle key.
  - Insert index entry pointing to L2 into parent of L.

To split inner node, redistribute entries evenly, but push up middle key.
*** 7.3.2 Delete
Start at root, find leaf L where entry belongs.

Remove the entry.
- If L is at least half-full, done!
- If L has only M/2-1 entries,
  - Try to re-distribute, borrowing from sibling(adjacent node with same parent as L).
  - If re-distribution fails, merge L and sibling.

If merge occurred, must delete entry(pointing to L or sibling) from parent of L.
*** 7.3.3 B+ Tree Visualization
https://cmudb.io/btree
** 7.4 Clustered Indexes
The table is stored in the sort order specified by the primary key.
- Can be either heap- or index-organized storage.

Some DBMSs always use a clustered index.
- If a table doesn't include a pkey, the DBMS will automatically make a hidden row id key.

Other DBMSs cannot use them at all.
** 7.5 Selection Conditions
The DBMS can use a B+ Tree index if the query provides any of the attributes of the search key.

Example: Index on <a, b, c>
- Supported: (a=5 AND b=3)
- Supported: (b=3).

Not all DBMSs support this.

For hash index, we must have all attributes in search key.
** 7.6 B+ Tree Design Choices
*** 7.6.1 Node Size
The slower the disk, the larger the optimal node size for B+ Tree.
- HDD: ~1MB
- SSD: ~10KB
- In-Memory: ~512B

Optimal sizes can vary depending on the workload
- Leaf Node Scans vs. Root-to-Leaf Traversals.
*** 7.6.2 Merge Threshold
Some DBMSs don't always merge nodes when it is half full.

Delaying a merge operation may reduce the amount of reorganization.

May be better to just let underflows to exist and then periodically rebuild entire tree.
*** 7.6.3 Variable Length Keys
Approach #1: Pointers
- Store the keys as pointers to the tuple's attribute.

Approach #2: Variable Length Nodes
- The size of each node in the B+ Tree can vary.
- Requires careful memory management.

Approach #3: Key Map
- Embed an array of pointers that map to the key + value list within the node.
*** 7.6.4 Non-unique Indexes
Approach #1: Duplicate Keys
- Use the same leaf node layout but store duplicate keys multiple times.

Approach #2: Value Lists
- Store each key only once and maintain a linked list of unique values.
*** 7.6.5 Intra-node Search
Approach #1: Linear
- Scan node keys from beginning to end.

Approach #2: Binary
- Jump to middle key, pivot left/right depending on comparsion.

Approach #3: Interpolation
- Approaximate location of desired key based on known distribution of keys.

** 7.7 Optimizations
*** 7.7.1 Prefix Compression
Stored keys in the same leaf node are likely to have the same prefix.

Instead of storing the entire key each time, extract common prefix and store only unique suffix for each key.
- Many variations.
*** 7.7.2 Suffix Truncation
The keys in the inner nodes are only used to "direct traffic".
- We don't actually need the entire key.

Store a minimum prefix that is needed to correctly route probes into the index.
*** 7.7.3 Bulk Insert
The fastest/best way to build a B+ Tree is to first sort the keys and then build the index from the bottom up.
*** 7.7.4 Pointer Swizzling
Nodes use page ids to reference other nodes in the index. The DBMS has to get the memory location from the page table during travesal.

If a page is pinned in the buffer pool, then we can store raw pointers instead of page ids, thereby removing the need to get address from the page table.
* 8 Tree Indexes Part II
** 8.1 Additional Index Usage
*** 8.1.1 Implicit Indexes
Most DBMSs automatically create an index to enforce integrity constraints.
- Primary Keys
- Unique Constraints
- Foreign Keys
*** 8.1.2 Partial Indexes
Create an index on subset of entire table. This potentially reduces the size of indexes and the amount of overhead to maintain it.

One common use case is to partition indexes by date ranges.
- Create a separate index per month, year.

#+begin_src sql
CREATE INDEX idx_foo
          ON foo (a, b)
       WHERE c = 'WuTang';
#+end_src
*** 8.1.3 Covering Indexes
If all of the fields needed to process the query are available in an index, then the DBMS doesn't need to retrieve the whole tuple.

This reduces contention on DBMS's buffer pool resources.

#+begin_src sql
CREATE INDEX idx_foo
          ON foo (a, b);

SELECT b FROM foo
 WHERE a = 123;
#+end_src
*** 8.1.4 Index Include Columns
Embed addtional columns in indexes to support index-only queries. Not a part of search key. This stores addtional columns in leaf node.

#+begin_src sql
CREATE INDEX idx_foo
          ON foo (a, b)
     INCLUDE (c);
#+end_src
*** 8.1.5 Functional/Expression Indexes
The index does not need to store keys in the same way that they appear in their base table.

You can use expressions when declaring an index.

#+begin_src sql
CREATE INDEX idx_user_login
    ON users (EXTRACT(dow FROM login));

SELECT * FROM users
 WHERE EXTRACT(dow FROM login) = 2;
#+end_src
** 8.2 Skip Lists
Multiple levels of linked lists with extra pointers that skip over intermediate nodes. Maintain keys in sorted order without requiring global rebalancing.

A collection of lists at different levels
- Lowest level is a sorted, single linked list of all keys.
- 2nd level links every other key.
- 3rd level links every fourth key.
- In general, a level has half the keys of one below it.

To insert a key, flip a coin to decide how many levels to add the new key into. Provides approximately O(log n) search time complexity.

Advantages:
- Uses less memory than a typical B+ Tree if you don't include reverse pointers.
- Insertions and deletions do not require rebalancing.

Disadvantages:
- Not disk/cache friendly because they do not optimize locality of references.
- Reverse search is non-trivial.
** 8.3 Radix tree
Represent keys as individual digits. This allow threads to examine prefix one-by-one instead of comparing entire key.
- The height of the tree depends on the length of the key.
- Does not require rebalancing.
- The path to a leaf node represents the key of the leaf.
- Keys are stored implicitly and can be reconstructed from paths.
*** 8.3.1 Radix Tree: Binary Comparable Keys
Not all attribute types can be composed into binary comparable digits for a radix tree.
- Unsigned Integers: Byte order must be flipped for little endian machines.
- Signed Integers: Flip two's-complement so that negative numbers are smaller than positive.
- Floats: Classify into group (neg vs. pos, normalized vs. denormalized), then store as unsigned integer.
- Compound: Transform each attribute separately.
** 8.4 Inverted Index
An inverted index stores a mapping of words to records that contain those words in the target attribute.
- Sometimes called a full-text search index.
* 9 Index Concurrency Control
A concurrency control protocol is the method that the DBMS uses to ensure "correct" results for concurrent operations on a shard object.
** 9.1 Locks VS. Latches
Locks
- Protects the index's logical contents from other txns.
- Held for txn duration.
- Need to be able to rollback changes.

Latches
- Protects the critical sections of the index's internal data structure from other threads.
- Held for operation duration.
- Do not need to be able to rollback changes.

|          | Locks                                | Latches                   |
|----------+--------------------------------------+---------------------------|
| Separate | User transaction                     | Threads                   |
| Protect  | Database Contents                    | In-Memory Data Structures |
| During   | Entire Transactions                  | Critical Sections         |
| Modes    | Shared, Exclusive, Update, Intention | Read, Write               |
| Deadlock | Detection & Resolution               | Avoidance                 |
| by       | Waits-for, Timeout, Aborts           | Coding Discipline         |
| Kept in  | Lock Manager                         | Protected Data Structure  |

Read Mode
- Multiple threads are allowed to read the same item at the same time.
- A thread can acquire the read latch if another thread has it in read mode.

Write Mode
- Only one thread is allowed to access the item.
- A thread cannot acquire a write latch is another thread holds the latch in any mode.
** 9.2 B+ Tree Concurrency Control
We want to allow multiple threads to read and update a B+ tree at the same time.

We need to protect from two types of problems:
- Threads trying to modify the contents of a node at the same time.
- One thread traversing the tree while another thread splits/merges nodes.
*** 9.2.1 Latch Crabbing/Coupling
Protocol to allow multiple threads to access/modify B+ Tree at the same time.

Basic Idea:
- Get latch for parent.
- Get latch for child.
- Release latch for parent if "safe".

A safe node is one that will not split or merge when updated.
- Not full (on insertion)
- More than half-full (on deletion)

Search: Start at root and go down; repeatedly,
- Acquire R latch on child
- Then unlatch parent

Insert/Delete: Start at root and go down, obtaining W latches as needed. Once child is latched, check if it is safe:
- If child is safe, release all latches on ancestors.
*** 9.2.2 Better Latching Algorithm
Assume that the leaf node is safe.

Use read latches and crabbing to reach it, and verify that it is safe.

If leaf is not safe, then do previous algorithm using write latches.

Search: Same as before.

Insert/Delete:
- Set latches as if for search, get to leaf, and set W latch on leaf.
- If leaf is not safe, release all latches, and restart thread using the previous insert/delete protocol with write latches.

This approach optimistically assumes that only leaf node will be modified; if not, R latches set on the first pass to leaf are wasteful.
*** 9.2.3 Leaf Node Scans
Latches do not support deadlock detection or avoidance. The only way we can deal with this problem is through coding discipline.

The leaf node sibling latch acquisition protocol must support a "no-wait" mode. B+ tree code must cope with failed latch acquisitions.
*** 9.2.4 Delayed Parent Updates
Every time a leaf node overflows, we have to update at least three nodes.
- The leaf node being split.
- The new leaf node being created.
- The parent node.

Blink Tree Optimization: When a leaf node overflows, delay updating its parent node.
* 10 Query Processing
** 10.1 Query Plan
The operators are arranged in a tree. Data flows from the leaves toward the root.

The ouput of the root node is the result of the query.
** 10.2 Processing Model
A DBMS's processing model defines how the system executes a query plan.
- Different trade-offs for different workloads.

Three approaches:
- Iterator model
- Materialization model
- Vectorized / Batch model

*** 10.2.1 Iterator Model
Each query plan operator implements a next function.
- On each invocation, the operator returns either a single tuple or a null marker if there are no more tuples.
- The operator implements a loop that calls next on its children to retrieve their tuples and then process them.

Top-down plan processing. Also called Volcano or Pipeline Model.

This is used in almost every DBMS. Allows for tuple pipelining.

Some operators will block until children emit all of their tuples.
- Joins, Subqueries, Order by.

Output control works easily with this approach.
- Limit.
*** 10.2.2 Materialization Model
Each operator processes its input all at once and then emits its output all at once.
- The operator "materializes" its output as a single result.
- The DBMS can push down hints into to avoid scanning too many tuples.

Bottom-up plan processing.

Better for OLTP workloads because queries typically only access a small number of tuples at a time.
- Lower execution / coordination overhead.

Not good for OLAP queries with large intermediate results.
*** 10.2.3 Vectorization Model
Like iterator model, each operator implements a next function.

Each operator emits a batch of tuples instead of a single tuple.
- The operator's internal loop processes multiple tuples at a time.
- The size of batch can vary based on hardware or query properties.

Ideal for OLAP queries
- Greatly reduces the number of invocations per operator.
- Allows for operators to use vectorized (SIMD) instructions to process batches of tuples.
*** 10.2.4 Summary
|           | Iterator/Volcano | Vectorized  | Materialization  |
|-----------+------------------+-------------+------------------|
| Direction | Top-Down         | Top-Down    | Bottom-Up        |
| Emits     | Single Tuple     | Tuple Batch | Entire Tuple Set |
| Target    | General Purpose  | OLAP        | OLTP             |
** 10.3 Access Methods
An access method is a way that the DBMS can access the data stored in a table.
- Not defined in relational algebra.

Three basic approaches:
- Sequential scan.
- Index scan.
- Multi-index / "Bitmap" scan.
*** 10.3.1 Sequential Scan
For each page in the table:
- Retrieve it from the buffer pool.
- Iterate over each tuple and check whether to include it.

The DBMS maintains an internal cursor that tracks the last page / slot it examined.

This is almost the worst thing that the DBMS can do to execute a query.

Sequential scan optimizations:
- Prefetching
- Parallelization
- Buffer pool bypass
- Zone maps
- Late materialization
- Heap clustering
**** 10.3.1.1 Zone Maps
Pre-computed aggregates for the attribute values in a page. DBMS checks the zone map first to decide whether it wants to access the page.
**** 10.3.1.2 Late Materialization
DSM DBMSs can delay stitching together tuples until the upper parts of the query plan.
**** 10.3.1.3 Heap Clustering
Tuples are sorted in the heap's pages using the order specified by a clustering index.

If the query accesses tuples using the clustering index's attributes, then the DBMS can jump directly to the pages that it needs.
*** 10.3.2 Index Scan
The DBMS picks an index to find the tuples that the query needs.

Which index to use depends on:
- What attributes the index contains
- What attributes the query references
- The attribute's value domains
- Predicate composition
- Whether the index has unique or non-unique keys
*** 10.3.3 Multi-index Scan
If there are multiple indexes that the DBMS can use for a query:
- Compute sets of record ids using each matching index.
- Combine these sets based on the query's predicates (union vs. intersect).
- Retrieve the records and apply any remaining terms.

Set intersection can be done with bitmaps, hash tables, or Bloom filters.
** 10.4 Index Scan Page Sorting
Retrieving tuples in the order that appear in an unclustered index is inefficient.

The DBMS can first figure out all the tuples that it needs and then sort them based on their page id.
** 10.5 Expression evaluation
The DBMS represents a WHERE clause as an expression tree.

The nodes in the tree represent different expression types:
- Comparisons (= , < , > , !=)
- Conjunction (AND), Disjunction (OR)
- Arithmetic Operators (+, -, *, /, %)
- Constant Values
- Tuple Attribute References

To evaluate an expression tree at runtime, the DBMS maintains a context handle that contains metadata for the execution, such as the current tuple, the parameters, the parameters, and the table schema. The DBMS then walks the tree to evaluate its operators and produce a result.
* 11 Sorting & Aggregations
Why do we need to sort?

Tuples in a table have no specific order.

But users often want to retrieve tuples in a specific order.
- Trivial to support duplicate elimination (DISTINCT)
- Bulk loading sorted tuples into a B+ tree index is faster
- Aggregations (GROUP BY)
** 11.1 Sorting Algorithms
*** 11.1.1 External Merge Sort
Sorting Phase
- Sort small chunks of data that fit in main-memory, and then write back the sorted data to a file on disk.

Merge Phase
- Combine sorted sub-files into a single larger file.

Files are broken up into N pages. The DBMS has a finite number of B fixed-size buffers.
**** 11.1.1.1 Two-way External Merge Sort
Pass #0
- Read every B pages of the table into memory.
- Sorts them, and write them back to disk.
- Each sorted set of pages is called a run.

Pass #1,2,3,...
- Recursively merges pairs of runs into runs twice as long.
- Use three buffer pages (2 for input pages, 1 for output).
**** 11.1.1.2 General External Merge Sort
Pass #0
- Use B buffer pages.
- Produce ceiling(N/B) sorted runs of size B

Pass #1,2,3,...
- Merge B-1 runs (i.e. K-way merge)

Number of passes = 1 + logB-1(ceiling(N/B))

a_{1}

- Use B+ tree
  - Good for clustered B+ tree
  - For unclustered B+ tree, this is almost always a bad idea. In general, one I/O per data record. The DBMS will choose the best one for us. Whether to use index, or do external merge sort.
** Aggregation
*** Sort
*** Hashing
Hashing is a better alternative in this scenario.
- Only need to remove duplicates, no need for ordering.
- Can be computationally cheaper than sorting.
* Join algorithms
We normalize tables in a relational database to avoid unnecessary repetition of information.

We use the join operate to reconstruct the original tuples without any information loss.

In general, we want the smaller table to always be the outer table.

** Nested Loop Join
Summary:
- Pick the smaller table as the outer table.
- Buffer as much of the outer table in memory as possible.
- Loop over the inner table or use an index.

*** Simple
#+begin_src C
foreach tuple r in R: (outer table)
  foreach tuple s in S: (inner table)
    emit, if r and s match
#+end_src
The outer table has M pages and m tuples. The inner table has N pages and n tuples.

So the cost is: M + (m \star{} N). In most cases, if we use the smaller table as the outer table, it will reduce the IO cost.

*** Block
Because the buffer pool reads tuples from the disk as pages. So we can do nested loop inside two pages first.

#+begin_src C
foreach block Br in R:
  foreach block Bs in S:
    foreach tuple r in Br:
      foreach tuple s in Bs:
        emit, if r and s mathch
#+end_src

This algorithm performs fewer disk accesses.
- For every block in R, it scans S once.

Cost: M + (M \star{} N)

Also, the smaller table in terms of # of pages should be the outer table.

What if we have B buffers available?
- Use B-2 buffers for scanning the outer table.
- Use one buffer for the inner table, one buffer for sorting output.

This algorithm uses B-2 buffers for scanning R.

Cost: M + (M / (B - 2) \star{} N)

*** Index
Use an index to find inner table matches to accelerate the join.
- We could use an existing index for the join.
- Or even build one on the fly.

#+begin_src C
foreach tuple r in R:
  foreach tuple s in Index(ri = sj):
    emit, if r and s match
#+end_src

Assume the cost of each index probe is some constant C per tuple.

Cost: M + (m \star{} C)

** Sort-Merge Join
Phase #1: Sort
- Sort both tables on the join key(s).
- Can use the external merge sort algorithm that we talked about last clase.

Phase #2: Merge
- Step through the two sorted tables in parallel, and emit matching tuples.
- May need to backtrack depending on the join type.

Sort Cost (R): 2M \times{} (log M / log B)

Sort Cost (S): 2N \times{} (log N / log B)

Merge Cost: M + N

When is sort-merge join useful?
- One or both tables are already sorted on join key.
- Output must be sorted on join key.

The input relations may be sorted by either by an explicit sort operator, or by scanning the relation using an index on the join key.
** Hash Join
Phase #1: Build
- Scan the outer relation and populate a hash table using the hash function h1 on the join attributes.

Phase #2: Probe
- Scan the inner relation and use h1 on each tuple to jump to a location in the hash table and find a matching tuple.

*** Hash table contents
Key: The attribute(s) that the query is joining the tables on.

Value: Varies per implementation.
- Depends on what the operators above the join in the query plan expect as its output.

Approach #1: Full Tuple
- Avoid having to retrieve the outer relation's tuple contents on a match.
- Takes up more space in memory.

Approach #2: Tuple Identifier
- Ideal for column stores because the DBMS doesn't fetch data from disk it doesn't need.
- Also better if join selectivity is low.

** Conclusion
Hashing is almost always better than sorting for operator execution.

Caveats:
- Sorting is better on non-uniform data.
- Sorting is better when result needs to be sorted.

Good DBMSs use either or both.
* Query Optimization
Remember that SQL is declarative.
- User tells the DBMS what answer they want, not know how to get the answer.

** Two approaches
Heuristics / Rules (written by human)
- Rewrite the query to remove stupid / inefficient things.
- Does not require a cost model.

Cost-based Search
- Use a cost model to evalute multiple equivalent plans and pick the one with the lowest cost.

** Query planning overview
SQL Query -> Parser -> (Abstract Syntax Tree) -> Binder (Name -> Internal ID) -> (Annotated AST) -> Rewriter -> Optimizer.

** Relational algebra equivalences
Two relational algebra expressions are equivalent if they generate the same set of tuples.

The DBMS can identify better query plans without a cost model. This is often called *query rewriting*.

Selections:
- Perform filters as early as possible.
- Reorder predicates so that the DBMS applies the most selective one first.
- Break a complex predicate, and push down.

Projections:
- Perform them early to create smaller tuples and reduce intermediate results (if duplicates are eliminated).
- Project out all attributes except the ones requested or required (e.g., joining keys).

This is not important for a column store.

Joins:
- Commutative, associative.

** Cost estimation
How long will a query take?
- CPU: small cost; tough to estimate.
- Disk: # of block transfers.
- Memory: Amount of DRAM used.
- Network: # of messages.

So we need to keep some statistics.

The DBMS stores internal statistics about the tables, attributes, and indexes in its internal catalog.

Different systems update them at different times.

Manual invocations:
- Postgres/SQLite: ANALYZE
- Oracle/MySQL: ANALYZE TABLE
- SQL Server: UPDATE STATISTICS
- DB2: RUNSTATS

*** Statistics
For each relation R, the DBMS maintains the following information:
- Nr: Number of tuples in R.
- V(A, R): Number of distinct values for attribute A.

The selection cardinality SC(A, R) is the average number of records with a value for an attribute A given Nr / V(A, R).

*** Complex predicates
The selectity(sel) of a predicate P is the fraction of tuples that qualify.

Formula depends on type of predicate:
- Equality
- Range
- Negation
- Conjunction
- Disjunction

1. Equality

sel(A=constant) = SC(P) / V(A, R)

2. Range Query

sel(A>=a) = (Amax - a) / (Amax - Amin)

3. Negation Query

sel(not P) = 1 - sel(P)

4. Conjunction

sel(P1 \land{} P2) = sel(P1) \times{} sel(P2)

This assumes that the predicates are independent.

5. Disjunction

sel(P1 \lor{} P2) = sel(P1) + sel(P2) - sel(P1 \land{} P2)
*** Result size estimation for joins
General case: Rcols \lor{} Scols = {A} where A is not a key for either table.
- Match each R-tuple with S-tuples:
  estSize = Nr \times{} Ns / V(A, S)
- Symmetrically, for S:
  estSize = Nr \times{} Ns / V(A, R)

Overall:
  estSize = Nr \times{} Ns / max{V(A, S), V(A, R)}
* Parallel Execution
** Parallel VS Distributed
Paralle DBMSs:
- Nodes are physically close to each other.
- Nodes connected with high-speed LAN.

Distributed DBMSs:
- Nodes can be far from each other.
- Nodes connected using public network.
- Communication cost and problems cannot be ignored.

** Process Model
A DBMS's process model defines how the system is architected to support concurrent requests from a multi-user  application.

A worker is the DBMS component that is responsible for executing tasks on behalf of the client and returning the results.

*** Approach #1: Process per DBMS Worker
Each worker is a separate OS process.
- Relies on OS scheduler.
- Use shared-memory for global data structures.
- A process crash doesn't take down entire system.
- Examples: IBM DB2, Postgres, Oracle.
*** Approach #2: Process Pool
A worker uses any process that is free in a pool.
- Still relies on OS scheduler and shared memory.
- Bad for CPU cache locality.
- Examples: IBM DB2, Postgres(2015)
*** Approach #3: Thread per DBMS Worker
Single process with multiple worker threads.
- DBMS has to manage its own scheduling.
- May or may not use a dispatcher thread.
- Thread crash (may) kill the entire system.
- Examples: IBM DB2, MS SQL, MySQL, Oracle (2014)

**** Scheduling
For each query plan, the DBMS has to decide where, when, and how to execute it.
- How many tasks should it use?
- How many CPU cores should it use?
- What CPU core should the tasks execute on?
- Where should a task store its output?

The DBMS always knows more than operating system.

** Inter- VS Intra-Query Parallelism
*** Inter-Query: Different queries are executed concurrently.
- Increases throughput & reduces latency.

If queries are read-only, then this requires little coordination between queries.

If queries are updating the database at the same time, then this is hard to do this correctly.
- Need to provide the illusion of isolation

*** Intra-Query: Execute the operations of a single query in parallel.
- Decreases latency for long-running queries.

**** Approach #1: Intra-Operator (Horizontal)
Operators are decomposed into independent instances that perform the same function on different subsets of data.

The DBMS inserts an exchange operator into the query plan to coalesce results from children operators.
**** Approach #2: Inter-Operator (Vertical)
Operations are overlapped in order to pipeline data from one stage to the next without materialization. Also called pipelined parallelism.

** I/O Parallelism
Split the DBMS installation across multiple storage devices.
* Embedded Logic
** User-defined functions
A user-defined function (UDF) is a function written by the application developer that extends the system's functionality beyond its built-in operations.
- It takes in input arguments (scalars)
- Perform some computation
- Return a result (scalars, tables)
** Stored procedures
A stored procedure is a self-contained function that performs more complex logic inside of the DBMS.
- Can have many input/output parameters.
- Can modify the database table/structures.
- Not normally used within a SQL query.
** Stored procedures VS UDF
A UDF is meant to perform a subset of a read-only computation within a query.

A stored procedure is meant to perform a complete computation that is independent of query.
** Database triggers
A trigger instructs the DBMS to invoke a UDF when some event occurs in the database.

The developer has to define:
- What type of event will cause it to fire.
- The scope of the event.
- When it fires relative to that event.

Event type:
- INSERT
- UPDATE
- DELETE
- TRUNCATE
- CREATE
- ALTER
- DROP

Event scope:
- TABLE
- DATABASE
- VIEW
- SYSTEM

Trigger timing:
- Before the statement executes.
- After the statement executes.
- Before each row that the statement affects.
- After each row that the statement affects.
- Instead of the statement.
** Change notifications
A change notification is like a trigger except that the DBMS sends a message to an external entity that something notable has happened in the database.
- Think a "pub/sub" system.
- Can be chained with a trigger to pass along whenever a change occurs.

SQL standard: LISTEN + NOTIFY.
** Complex types
Approach #1: Attribute splitting
- Store each primitive element in the complex type as its own attribute in the table.

Approach #2: Application serialization
- Java serialize, Python pickle
- Google protobuf, Facebook thrift
- JSON / XML
** User-defined types
A user-defined type is a special data type that is defined by the application developer that the DBMS can stored natively.
** Views
Create a "virtual" table containing the output from a SELECT query. The view can then be accessed as if it was a real table.

This allows programmers to simplify a complex query that is executed often.
- Won't make it faster though.

Often used as mechanism for hiding a subset of a table's attributes from certain users. The DBMS will rewrite the SQL.
** Views VS Select Into
VIEW
- Dynamic results are only materialized when needed.

SELECT...INTO
- Creates static table that does not get updated when student gets updated.

MATERIALIZED VIEWS
Create a view containing the output from a SELECT query that is automatically updated when the underlying tables change.
* Concurrency Control Theory
A DBMS's concurrency control and recovery components permeate throughout the design of its entire architecture.
** Motivation
We both change the same record in a table at the same time.

How to avoid race condition?

You transfer $100 between bank accounts but there is a power failure.

What is the correct database state?
** Transactions
A transaction is the execution of a sequence of one or more operations (e.g., SQL queries) on a shared database to perform some higher-level function.

It is the basic unit of change in a DBMS:
- Partial transactions are not allowed!
** Formal Definitions
Database: A fixed set of named data objects(e.g., A, B, C, ...)
- We don't need to define what these objects are now.

Transaction: A sequence of read and write operations (R(A), W(B), ...)
- DBMS's abtract view of a user program.

*** Transactions in SQL
A new txn starts with the BEGIN command.

The txn stops with either COMMIT or ABORT:
- If commit, all changes are saved.
- If abort, all changes are undone so that it's like as if the txn never executed at all.
- Abort can be either self-inflicted or caused by the DBMS.
** Correctness Criteria: ACID
Atomicity: All actions in the txn happen, or none happen. "all or nothing".

Consistency: If each txn is consistent and the DB starts consistent, then it ends up consistent. "it looks correct to me".

Isolation: Execution of one txn is isolated from that of other txns. "as if alone".

Durability: If a txn commits, its effect persist. "survive failures".
*** Mechanisms for Ensuring Atomicity
Approach #1: Logging
- DBMS logs all actions so that it can undo the actions of aborted transactions.

Approach #2: Shadow Paging
- DBMS makes copies of pages and txns make changes to those copies. Only when the txn commits is the page made visible to others.
*** Consistency
**** Database Consistency
The database accurately models the real world and follows integrity constraints.

Transactions in the future see the effects of transactions committed in the past inside of the database.
**** Transaction Consistency
If the database is consistent before the transaction starts (running alone), it will also be consistent after.

Transaction consistency is the application's responsibility.
*** Isolation of Transactions
Users submit txns, and each txn executes as if it was running by itself.

Concurrency is achieved by DBMS, which interleaves actions (reads/writes of DB objects) of various transactions.
**** Mechanisms for Ensuring Isolation
A concurrency control protocol is how the DBMS decides the proper interleaving of operations from multiple transactions.

Two categories of protocols:
- Pessimistic: Don't let problems arise in the first place.
- Optimistic: Assume conflicts are rare, deal with them after they happen.
**** Correctness
How do we judge whether a schedule is correct?

If the schedule is equivalent to some serial execution.
**** Execution Schedule
- Serial Schedule
- Equivalent Schedules
- Serializable Schedule
**** Conflicts
- Read-Write Conflicts ("Unrepeatable Reads")
- Write-Read Conflicts ("Dirty Reads")
- Write-Write Conflicts ("Lost Updates")
**** Serializability
- Conflict Serializability
- View Serializability
*** Transaction Durability
All of the changes of committed transactions should be persistent.
- No torn updates.
- No changes from failed transactions.

The DBMS use either logging or shadow paging to ensure that all changes are durable.
